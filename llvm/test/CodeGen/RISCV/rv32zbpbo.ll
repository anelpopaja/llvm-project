; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
; RUN:   | FileCheck %s -check-prefix=RV32I
; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbpbo -verify-machineinstrs < %s \
; RUN:   | FileCheck %s -check-prefix=RV32ZBPBO

declare i32 @llvm.ctlz.i32(i32, i1)

define i32 @ctlz_i32(i32 %a) nounwind {
; CHECK-LABEL: ctlz_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; CHECK-NEXT:    beqz a0, .LBB0_2
; CHECK-NEXT:  # %bb.1: # %cond.false
; CHECK-NEXT:    srli a1, a0, 1
; CHECK-NEXT:    or a0, a0, a1
; CHECK-NEXT:    srli a1, a0, 2
; CHECK-NEXT:    or a0, a0, a1
; CHECK-NEXT:    srli a1, a0, 4
; CHECK-NEXT:    or a0, a0, a1
; CHECK-NEXT:    srli a1, a0, 8
; CHECK-NEXT:    or a0, a0, a1
; CHECK-NEXT:    srli a1, a0, 16
; CHECK-NEXT:    or a0, a0, a1
; CHECK-NEXT:    not a0, a0
; CHECK-NEXT:    srli a1, a0, 1
; CHECK-NEXT:    lui a2, 349525
; CHECK-NEXT:    addi a2, a2, 1365
; CHECK-NEXT:    and a1, a1, a2
; CHECK-NEXT:    sub a0, a0, a1
; CHECK-NEXT:    lui a1, 209715
; CHECK-NEXT:    addi a1, a1, 819
; CHECK-NEXT:    and a2, a0, a1
; CHECK-NEXT:    srli a0, a0, 2
; CHECK-NEXT:    and a0, a0, a1
; CHECK-NEXT:    add a0, a2, a0
; CHECK-NEXT:    srli a1, a0, 4
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    lui a1, 61681
; CHECK-NEXT:    addi a1, a1, -241
; CHECK-NEXT:    and a0, a0, a1
; CHECK-NEXT:    lui a1, 4112
; CHECK-NEXT:    addi a1, a1, 257
; CHECK-NEXT:    call __mulsi3@plt
; CHECK-NEXT:    srli a0, a0, 24
; CHECK-NEXT:    j .LBB0_3
; CHECK-NEXT:  .LBB0_2:
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:  .LBB0_3: # %cond.end
; CHECK-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
; RV32I-LABEL: ctlz_i32:
; RV32I:       # %bb.0:
; RV32I-NEXT:    beqz a0, .LBB0_2
; RV32I-NEXT:  # %bb.1: # %cond.false
; RV32I-NEXT:    addi sp, sp, -16
; RV32I-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32I-NEXT:    srli a1, a0, 1
; RV32I-NEXT:    or a0, a0, a1
; RV32I-NEXT:    srli a1, a0, 2
; RV32I-NEXT:    or a0, a0, a1
; RV32I-NEXT:    srli a1, a0, 4
; RV32I-NEXT:    or a0, a0, a1
; RV32I-NEXT:    srli a1, a0, 8
; RV32I-NEXT:    or a0, a0, a1
; RV32I-NEXT:    srli a1, a0, 16
; RV32I-NEXT:    or a0, a0, a1
; RV32I-NEXT:    not a0, a0
; RV32I-NEXT:    srli a1, a0, 1
; RV32I-NEXT:    lui a2, 349525
; RV32I-NEXT:    addi a2, a2, 1365
; RV32I-NEXT:    and a1, a1, a2
; RV32I-NEXT:    sub a0, a0, a1
; RV32I-NEXT:    lui a1, 209715
; RV32I-NEXT:    addi a1, a1, 819
; RV32I-NEXT:    and a2, a0, a1
; RV32I-NEXT:    srli a0, a0, 2
; RV32I-NEXT:    and a0, a0, a1
; RV32I-NEXT:    add a0, a2, a0
; RV32I-NEXT:    srli a1, a0, 4
; RV32I-NEXT:    add a0, a0, a1
; RV32I-NEXT:    lui a1, 61681
; RV32I-NEXT:    addi a1, a1, -241
; RV32I-NEXT:    and a0, a0, a1
; RV32I-NEXT:    lui a1, 4112
; RV32I-NEXT:    addi a1, a1, 257
; RV32I-NEXT:    call __mulsi3@plt
; RV32I-NEXT:    srli a0, a0, 24
; RV32I-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32I-NEXT:    addi sp, sp, 16
; RV32I-NEXT:    ret
; RV32I-NEXT:  .LBB0_2:
; RV32I-NEXT:    li a0, 32
; RV32I-NEXT:    ret
;
; RV32ZBPBO-LABEL: ctlz_i32:
; RV32ZBPBO:       # %bb.0:
; RV32ZBPBO-NEXT:    beqz a0, .LBB0_2
; RV32ZBPBO-NEXT:  # %bb.1: # %cond.false
; RV32ZBPBO-NEXT:    clz a0, a0
; RV32ZBPBO-NEXT:    ret
; RV32ZBPBO-NEXT:  .LBB0_2:
; RV32ZBPBO-NEXT:    li a0, 32
; RV32ZBPBO-NEXT:    ret
  %1 = call i32 @llvm.ctlz.i32(i32 %a, i1 false)
  ret i32 %1
}

define i32 @pack_i32(i32 %a, i32 %b) nounwind {
; RV32I-LABEL: pack_i32:
; RV32I:       # %bb.0:
; RV32I-NEXT:    slli a0, a0, 16
; RV32I-NEXT:    srli a0, a0, 16
; RV32I-NEXT:    slli a1, a1, 16
; RV32I-NEXT:    or a0, a1, a0
; RV32I-NEXT:    ret
;
; RV32ZBPBO-LABEL: pack_i32:
; RV32ZBPBO:       # %bb.0:
; RV32ZBPBO-NEXT:    pkbb16 a0, a0, a1
; RV32ZBPBO-NEXT:    ret
  %shl = and i32 %a, 65535
  %shl1 = shl i32 %b, 16
  %or = or i32 %shl1, %shl
  ret i32 %or
}

define i32 @packu_i32(i32 %a, i32 %b) nounwind {
; RV32I-LABEL: packu_i32:
; RV32I:       # %bb.0:
; RV32I-NEXT:    srli a0, a0, 16
; RV32I-NEXT:    lui a2, 1048560
; RV32I-NEXT:    and a1, a1, a2
; RV32I-NEXT:    or a0, a1, a0
; RV32I-NEXT:    ret
;
; RV32ZBPBO-LABEL: packu_i32:
; RV32ZBPBO:       # %bb.0:
; RV32ZBPBO-NEXT:    pktt16 a0, a0, a1
; RV32ZBPBO-NEXT:    ret
  %shr = lshr i32 %a, 16
  %shr1 = and i32 %b, -65536
  %or = or i32 %shr1, %shr
  ret i32 %or
}

define i32 @max_i32(i32 %a, i32 %b) nounwind {
; RV32I-LABEL: max_i32:
; RV32I:       # %bb.0:
; RV32I-NEXT:    blt a1, a0, .LBB3_2
; RV32I-NEXT:  # %bb.1:
; RV32I-NEXT:    mv a0, a1
; RV32I-NEXT:  .LBB3_2:
; RV32I-NEXT:    ret
;
; RV32ZBPBO-LABEL: max_i32:
; RV32ZBPBO:       # %bb.0:
; RV32ZBPBO-NEXT:    max a0, a0, a1
; RV32ZBPBO-NEXT:    ret
  %cmp = icmp sgt i32 %a, %b
  %cond = select i1 %cmp, i32 %a, i32 %b
  ret i32 %cond
}

define i32 @min_i32(i32 %a, i32 %b) nounwind {
; RV32I-LABEL: min_i32:
; RV32I:       # %bb.0:
; RV32I-NEXT:    blt a0, a1, .LBB4_2
; RV32I-NEXT:  # %bb.1:
; RV32I-NEXT:    mv a0, a1
; RV32I-NEXT:  .LBB4_2:
; RV32I-NEXT:    ret
;
; RV32ZBPBO-LABEL: min_i32:
; RV32ZBPBO:       # %bb.0:
; RV32ZBPBO-NEXT:    min a0, a0, a1
; RV32ZBPBO-NEXT:    ret
  %cmp = icmp slt i32 %a, %b
  %cond = select i1 %cmp, i32 %a, i32 %b
  ret i32 %cond
}

declare i16 @llvm.bswap.i16(i16)

define zeroext i16 @bswap_i16(i16 zeroext %a) nounwind {
; RV32I-LABEL: bswap_i16:
; RV32I:       # %bb.0:
; RV32I-NEXT:    srli a1, a0, 8
; RV32I-NEXT:    slli a0, a0, 8
; RV32I-NEXT:    or a0, a0, a1
; RV32I-NEXT:    slli a0, a0, 16
; RV32I-NEXT:    srli a0, a0, 16
; RV32I-NEXT:    ret
;
; RV32ZBPBO-LABEL: bswap_i16:
; RV32ZBPBO:       # %bb.0:
; RV32ZBPBO-NEXT:    rev8.h a0, a0
; RV32ZBPBO-NEXT:    ret
  %1 = tail call i16 @llvm.bswap.i16(i16 %a)
  ret i16 %1
}

define i32 @cmix_i32(i32 %a, i32 %b, i32 %c) nounwind {
; RV32I-LABEL: cmix_i32:
; RV32I:       # %bb.0:
; RV32I-NEXT:    and a0, a1, a0
; RV32I-NEXT:    not a1, a1
; RV32I-NEXT:    and a1, a1, a2
; RV32I-NEXT:    or a0, a1, a0
; RV32I-NEXT:    ret
;
; RV32ZBPBO-LABEL: cmix_i32:
; RV32ZBPBO:       # %bb.0:
; RV32ZBPBO-NEXT:    cmix a0, a1, a0, a2
; RV32ZBPBO-NEXT:    ret
  %and = and i32 %b, %a
  %neg = xor i32 %b, -1
  %and1 = and i32 %neg, %c
  %or = or i32 %and1, %and
  ret i32 %or
}
